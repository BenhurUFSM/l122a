## Implementação de números

A tecnologia usada atualmente para implementar computadores digitais utiliza circuitos eletrônicos.
Os números que são manipulados por essas máquinas devem ser implementados usando grandezas que são processáveis por essas máquinas.
A forma mais simples que se encontrou para implementar isso foi usando uma representação em binário, em que um número é implementado por diversos dígitos, e cada dígito pode ter somente dois valores, usualmente representados por `0` e `1`.
A representação de um núemro em binário usa um mecanismo muito semelhanta ao que usamos para representar números na notação decimal: cada dígito tem um valor, que é alterado de acordo com a posição do dígito.

Em decimal, o dígito mais a direita tem peso 1, o dígito à esquerda dele tem peso 10, o seguinte à esquerda tem peso 100 e assim por diante.
Cada dígito tem o peso dez vezes maior que o do dígito à sua direita.

Em binário, usa-se o mesmo princípio, mas a base de mudança dos pesos é 2 e não 10. O dígito mais à direita tem peso 1, o que está à esquerda dele tem peso 2, o anterior tem peso 4, depois 8, 16, etc.
A sequência de dígitos `101` tem o valor cento e um se estiver em decimal e o valor cinco se estiver em binário.
Um dígito em binário é chamado de **bit**.

Para se poder representar uma grandeza útil, é necessário juntar vários desses bits, da mesma forma que se necessita de mais dígitos decimais quando se representa qualquer valor superior à 9.
Um computador tem uma determinada capacidade de armazenamento, ou uma certa quantidade de bits que ele consegue manter em sua memória.
Quanto mais bits se usa para representar um valor, menor é o número de valores que se consegue colocar em determinada memória de computador.
Para dar mais liberdade ao programador, os computadores permitem que se escolha, para cada valor que se vai colocar no computador, a quantidade de bits que vai ser usada (entre algumas configurações oferecidas).
Os computadores atuais geralmente oferecem 4 possibilidades, 8, 16, 32 ou 64 bits para um valor, ou 1, 2, 4 ou 8 **bytes** (um byte são 8 bits).
Com *n* bits se consegue representar uma gama de *2<sup>n</sup>* valores diferentes (da mesma forma que com *n* dígitos decimais se consegue representar *10<sup>n</sup>* valores diferentes).
